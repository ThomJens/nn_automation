{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91d70022-7d05-4945-bbfd-65833c7299da",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Install and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa62f643-f0dc-4d89-bedb-545399f67512",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mysql-connector\n",
    "!pip install pymysql\n",
    "!pip install ipython-sql\n",
    "!pip install mysql-connector-python\n",
    "!pip install sqlalchemy\n",
    "!pip3 install skorch\n",
    "\n",
    "# Data\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "# Database\n",
    "import sqlalchemy\n",
    "import pymysql.cursors\n",
    "import mysql\n",
    "from mysql.connector import Error\n",
    "from mysql.connector import errorcode\n",
    "import mysql.connector\n",
    "\n",
    "# System\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "import multiprocessing as mp\n",
    "import warnings\n",
    "import json \n",
    "from contextlib import redirect_stdout\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Neural network\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clear_output()\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7484d46b-f626-4546-ad06-225833ff87a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "115e7826-18da-45ce-ac18-c99f3ac84ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "        'host': '',\n",
    "        'port': ,\n",
    "        'user': '',\n",
    "        'password': '',\n",
    "        'database': '',\n",
    "        'use_pure': \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64556d3f-ead5-4f3c-a154-8b6d054ebdef",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4080b28-02a0-40fa-85f7-d5e48421e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDatabase(sql: str, columns) -> pd.DataFrame:\n",
    "    # Read the database using the CONFIG variable and returns a Dataframe holding the data from the table\n",
    "    conn = mysql.connector.connect(**CONFIG)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    df = pd.DataFrame(cursor.fetchall(), columns = columns)\n",
    "    conn.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def readColumns(sql: str) -> list:\n",
    "    # Read the database using the CONFIG variable and returns a Dataframe holding the tables column names\n",
    "    conn = mysql.connector.connect(**CONFIG)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    columns = [column[0] for column in cursor.fetchall()]\n",
    "    \n",
    "    return columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e45896-c820-4e52-9be3-c3e6f3d70402",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a93452-8769-4df4-9f55-21ad93d1d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model_name: str, train_model: nn.Sequential, parameters: dict = None, validate: bool = True, shuffle = False):\n",
    "    # Define default settings, that are replaced with GridSearchCS best_params_\n",
    "    best_parameters = {'lr': 0.005, 'batch_size': 256, 'optimizer': None}\n",
    "    # Database\n",
    "    database_train = 'Train_Dynamo'\n",
    "    database_test = 'Test_Dynamo'\n",
    "    train_rows = 2_000_000\n",
    "    test_rows = 1_000_000\n",
    "    target_column = 'Energy_Usage'\n",
    "    # Amount of epochs\n",
    "    epochs = 5\n",
    "    \n",
    "    # Calculates haw many chunks is the data split during training\n",
    "    chunks = math.ceil(1_000_000 / best_parameters['batch_size'])\n",
    "\n",
    "    # Set the model to use GPU if available, else CPU\n",
    "    model = train_model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Set the loss function and optimizer function\n",
    "    criterion = nn.MSELoss()\n",
    "    best_parameters['optimizer'] = torch.optim.Adagrad(model.parameters(), best_parameters['lr'])\n",
    "    \n",
    "    # Get the column names from the database\n",
    "    columns = readColumns(f'DESC {database_train};')\n",
    "    columns_test = readColumns(f'DESC {database_test};')\n",
    "\n",
    "    # Run the GridSearchCV\n",
    "    if parameters is not None:\n",
    "        # Read data from the database\n",
    "        df = readDatabase(f'SELECT * FROM {database_train} LIMIT {train_rows};', columns)\n",
    "        # X is the feature used for training\n",
    "        X = df.drop(['Energy_Usage'], axis = 1)\n",
    "        X = torch.FloatTensor(X.values)\n",
    "        # y is the target feature that the model is trying learn.\n",
    "        y = df['Energy_Usage']\n",
    "        y = torch.FloatTensor(y.values).reshape([-1, 1])\n",
    "        best_parameters = gridSearchNN(model_name, model, parameters, X, y)\n",
    "        best_parameters['optimizer'] = best_parameters['optimizer'](model.parameters(), best_parameters['lr'])\n",
    "\n",
    "    start =  datetime.now()\n",
    "    ###\n",
    "    ### Define variables used for evaluating the model\n",
    "    ###\n",
    "    # Define variables used for evaluating learning\n",
    "    running_loss = 0\n",
    "    valid_loss = 0\n",
    "    samples = 0\n",
    "    losses = []\n",
    "    validation_loss = []\n",
    "    # Define variables used for calculating errors\n",
    "    mse_list = []\n",
    "    mae_list = []\n",
    "    r2_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Define error and loss variables for epoch\n",
    "        epoch_time =  datetime.now()\n",
    "        epoch_loss = 0\n",
    "        predicts = []\n",
    "        labels = []\n",
    "\n",
    "        # Read data from database\n",
    "        partial_df = readDatabase(f'SELECT * FROM {database_train} LIMIT {train_rows};', columns)\n",
    "        if shuffle:\n",
    "            partial_df = partial_df.sample(frac=1)\n",
    "\n",
    "        for chunk in range(chunks):\n",
    "            # Checks if Dataframe is empty, to avoid errors\n",
    "            if partial_df.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            # Training the model\n",
    "            model.train()\n",
    "            best_parameters['optimizer'].zero_grad()\n",
    "            # Read data from the database\n",
    "            df = partial_df.iloc[best_parameters['batch_size'] * chunk:best_parameters['batch_size'] * chunk + best_parameters['batch_size']]\n",
    "            # X is the feature used for training\n",
    "            X = df.drop(['Energy_Usage'], axis = 1)\n",
    "            X = torch.FloatTensor(X.values)\n",
    "            # y is the target feature that the model is trying learn.\n",
    "            y = df['Energy_Usage']\n",
    "            y = torch.FloatTensor(y.values).reshape([-1, 1])\n",
    "\n",
    "            # Getting the prediction and loss.\n",
    "            # y_hat is the target prediction\n",
    "            y_hat = model.forward(X.to(device))\n",
    "            loss = criterion(y_hat, y.to(device))\n",
    "            running_loss += loss\n",
    "            epoch_loss += loss\n",
    "            samples += 1\n",
    "\n",
    "            loss.backward()\n",
    "            best_parameters['optimizer'].step()\n",
    "\n",
    "        # Calculate the loss and add it to the list\n",
    "        losses.append(running_loss / samples)\n",
    "    \n",
    "        # Validate the model after each epoch\n",
    "        if validate:\n",
    "            model.eval()\n",
    "            # Small memory manage action.\n",
    "            del partial_df\n",
    "            # No need for gradient calculation, since model isn't learning here.\n",
    "            with torch.no_grad():\n",
    "                # Read database and drop unnecessary column that was left behind.\n",
    "                test_df = readDatabase(f'SELECT * FROM Test_Dynamo LIMIT {test_rows};', columns_test)\n",
    "                #\n",
    "                ###\n",
    "                #### Remove if not necessary ###\n",
    "                test_df = test_df.drop(columns=\"index\", axis=1)\n",
    "                ### ######################## ###\n",
    "                ###\n",
    "                #\n",
    "                x = 0\n",
    "                while x < test_rows:\n",
    "                    # Read data from the database\n",
    "                    partial_df = test_df.iloc[x:x+best_parameters['batch_size']]\n",
    "                    # X is the feature used for training\n",
    "                    X = partial_df.drop([target_column], axis = 1)\n",
    "                    X = torch.FloatTensor(X.values)\n",
    "                    # y is the target feature that the model is trying learn.\n",
    "                    y = partial_df[target_column]\n",
    "                    y = torch.FloatTensor(y.values).reshape([-1, 1])\n",
    "\n",
    "                    # Getting the prediction and loss.\n",
    "                    # y_hat is the target prediction\n",
    "                    y_hat = model.forward(X.cuda())\n",
    "                    loss = criterion(y_hat, y.cuda())\n",
    "                    valid_loss += loss.item()\n",
    "                    x += best_parameters['batch_size']\n",
    "\n",
    "                    # Add the labels and prediction to list for later use.\n",
    "                    for j in range(len(y)):\n",
    "                        labels.append(y[j].item())\n",
    "                        predicts.append(y_hat[j].item())\n",
    "\n",
    "                    # Calulate errors\n",
    "                    mse = F.mse_loss(y_hat, y.cuda())\n",
    "                    mae = F.l1_loss(y_hat, y.cuda())\n",
    "                    r2 = r2_score(y.cuda().cpu().numpy(), y_hat.cuda().cpu().numpy())\n",
    "                    mse_list.append(mse)\n",
    "                    mae_list.append(mae)\n",
    "                    r2_list.append(r2)\n",
    "                \n",
    "                # Calculate validation loss and how many of the predictions are within acceptable range of the correct value.\n",
    "                validation_loss.append(valid_loss/(epoch*test_rows+test_rows))\n",
    "                threshold = len(compareResults(np.array(labels), np.array(predicts)))/len(labels)\n",
    "                \n",
    "        print(f'Time: {datetime.now() - epoch_time}\\tAccuracy: {threshold*100}%\\nTraining loss: {running_loss / samples}\\tValidation loss: {valid_loss/(epoch*test_rows+test_rows)}\\n')\n",
    "\n",
    "    print((datetime.now() - start))\n",
    "    # Create the name for the model and save it\n",
    "    opt_name = best_parameters['optimizer']._zero_grad_profile_name.split('#')[-1].split('.')[0]\n",
    "    lr = best_parameters['lr']\n",
    "    batch_size = best_parameters['batch_size']\n",
    "    model_name = f'{model_name}_{opt_name}_{lr}_{batch_size}'.replace('.', '')\n",
    "    model_name += '.pt'\n",
    "    torch.save(model, model_name)\n",
    "    #error_dict = {'mse': , 'mse':}\n",
    "    #errors = errorValues(labels, predicts, )\n",
    "    return losses, validation_loss, [mse_list, mae_list, r2_list], model_name, threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891bc3a-b995-470f-8a83-63ca8562339e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02fbd73a-d6de-4865-a342-95acebb15f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareResults(label: np.array, pred: np.array, error_margin: float = 0.05) -> np.array:\n",
    "    # Calculate the error percentage\n",
    "    return pred[abs(pred/label - 1) <= error_margin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a42ad8-78b5-4c33-8889-d86fa70977f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorValues(labels: list, predicts: list, error_dict: dict) -> dict:\n",
    "\n",
    "    error_list = np.absolute(labels - predicts)\n",
    "\n",
    "    # Calculate if guesses are within 5% of the correct values\n",
    "    Threshold_P = len(compareResults(labels, predicts))/len(labels)\n",
    "\n",
    "    # Calculate error values\n",
    "    Avg_Error = error_list.sum() / len(error_list)\n",
    "    Min_Error = error_list.min()\n",
    "    Max_Error = error_list.max()\n",
    "\n",
    "    # Calculate losses\n",
    "    MSE_Loss = (sum(error_dict['mse']) / len(error_dict['mse']))\n",
    "    MAE_Loss = (sum(error_dict['mae']) / len(error_dict['mae']))\n",
    "\n",
    "    # Calculate R2 score\n",
    "    R2_Score = (sum(error_dict['r2']) / len(error_dict['r2']))\n",
    "    \n",
    "    errors = {'Threshold_P': Threshold_P, 'Avg_Error': Avg_Error, 'Min_Error': Min_Error, 'Max_Error': Max_Error, 'MSE_Loss': MSE_Loss.item(), 'MAE_Loss': MAE_Loss.item(), 'r2': R2_Score}\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2914f095-2913-4ee7-a53c-91db8915bad6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec542164-7200-4230-98f5-b507887baf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearchNN(name: str, model: nn.Sequential, parameters: dict, X: torch.FloatTensor, y: torch.FloatTensor, max_epochs: int = 10, cv: int = 3, scoring: str = 'r2', refit : bool = False, verbose: int = 3) -> dict:\n",
    "    # Prepare the model for GridSearchCV\n",
    "    net = NeuralNetRegressor(model,\n",
    "                             max_epochs = max_epochs,\n",
    "                             lr = 0.01,\n",
    "                             verbose = verbose)\n",
    "    # Create a filename\n",
    "    time = str(datetime.now().time()).split('.')[0].replace(':', '')\n",
    "    date = str(datetime.now().date()).replace('-', '')\n",
    "    file_name = f'grid_log_{name}_{date}_{time}.txt'\n",
    "\n",
    "    # Running the GridSearchCV and logging it to the file\n",
    "    with open(file_name, 'w') as f:\n",
    "        with redirect_stdout(f):\n",
    "            print(name, '\\n')\n",
    "            gs = GridSearchCV(net, parameters, refit = refit, scoring = scoring, verbose = verbose, cv = cv)\n",
    "            gs.fit(X, y)\n",
    "\n",
    "    # Add additional information to the log\n",
    "    results_text = '\\n\\nResults\\n'\n",
    "    for key, value in gs.cv_results_.items():\n",
    "        results_text += f'{key}: {value}\\n'\n",
    "    with open(file_name, \"a\") as file:\n",
    "        file.write(results_text)\n",
    "        \n",
    "    return gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be393a3-9fe5-407f-b890-606b7cbaeda9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Neuron increments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25afb79f-bdd0-4cfe-9785-2b6bc583ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple means to increase/decrease the models neurons\n",
    "def addNeurons(previous: int, increment: int) -> int:\n",
    "    return previous + increment\n",
    "\n",
    "def multiplyNeurons(previous: int, increment: int) -> int:\n",
    "    return int(previous * increment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f440ecf0-666e-4d75-95d2-ec21538f0a88",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a832ee2-ae0a-43c6-b4cf-2ec9e861ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(input: int, output: int, inputs_output: int, method: str, layers: int, neuron_increment: int, ignore_ones = False) -> nn.Sequential:    \n",
    "    # Contains all the neuron increment functions with a keyword to call them\n",
    "    method_functions = {'add': addNeurons, 'mult': multiplyNeurons}\n",
    "    \n",
    "    # Checks if first layers neurons are greater than 0, If not, then uses increment function to set it\n",
    "    if inputs_output <= 0:\n",
    "        inputs_output = method_functions[method](input, neuron_increment)\n",
    "        \n",
    "    # Define the model and adds the input layer\n",
    "    model = nn.Sequential(nn.Linear(input, inputs_output))\n",
    "    # Keep track of the amount of neurons in previous layer\n",
    "    previous = inputs_output\n",
    "\n",
    "    # Build the model.\n",
    "    for layer in range(0, layers-2):\n",
    "        # Adds the activation function\n",
    "        model.append(nn.ReLU())\n",
    "        \n",
    "        # Calculates the input and output neurons for the next layer\n",
    "        next_neurons = method_functions[method](previous, neuron_increment)\n",
    "        # Checks Special cases and interupts the run to inform the user.\n",
    "        if next_neurons == 1 and not ignore_ones:\n",
    "            raise ValueError('Number of neurons reached 1 before output. If this was by design, use ignore_ones = True to ignore this.')\n",
    "        if next_neurons == 0:\n",
    "            raise ValueError('Number of neurons reached 0.')\n",
    "            \n",
    "        # Adds the next layer\n",
    "        model.append(nn.Linear(previous, next_neurons))\n",
    "        # Update  the neuron amount\n",
    "        previous = next_neurons\n",
    "    \n",
    "    # Adds the activation function and the output layer\n",
    "    model.append(nn.ReLU())\n",
    "    model.append(nn.Linear(previous, output))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8850de5-a221-4142-a9bd-c4590678a3c8",
   "metadata": {},
   "source": [
    "# Main function for finding the hyperparameters for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b44fd94-8941-4d4d-82fa-6e01f5659323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def networkLooper(nn_parameters: dict, grid_parameters: dict = None) -> None:\n",
    "    # Split the dictionary to variables to make it more readable\n",
    "    input = nn_parameters['input']\n",
    "    output = nn_parameters['output']\n",
    "    inputs_output = nn_parameters['inputs_output']\n",
    "    \n",
    "    methods = nn_parameters['method']    \n",
    "    layers = nn_parameters['layers']\n",
    "    neuron_increment = nn_parameters['neuron_increment']\n",
    "    \n",
    "    # Checks that variables are in list\n",
    "    if type(methods) != list:\n",
    "        methods = [methods]\n",
    "    if type(layers) != list:\n",
    "        layers = [layers]\n",
    "    if type(neuron_increment) != list:\n",
    "        neuron_increment = [neuron_increment]\n",
    "    \n",
    "    # Define the file name\n",
    "    time = str(datetime.now().time()).split('.')[0].replace(':', '')\n",
    "    date = str(datetime.now().date()).replace('-', '')\n",
    "    file_name = f'Log_{date}_{time}.txt'\n",
    "    with open(file_name, 'w') as file:\n",
    "        file.write('')\n",
    "        \n",
    "    # Creates a single list holding all the possible parameter variations for the model by using Cartesian Product\n",
    "    all_list = [methods, layers, neuron_increment]\n",
    "    param_combination = product(*all_list)\n",
    "\n",
    "    # Buildin a model, trainng it and logging the results\n",
    "    for param in param_combination:\n",
    "        # param[0] = method, param[1] = layers,param[2] = neurons\n",
    "        # Building the model itself and giving it a name\n",
    "        model = buildModel(input, output, inputs_output, *param)\n",
    "        name = f'{inputs_output}_{param[0]}_{param[1]}_{param[2]}'\n",
    "        print(name)\n",
    "        # Training the model\n",
    "        losses, validation_loss, errors, name, threshold = trainModel(name, model, grid_parameters)\n",
    "        # Preparing the losses for the file\n",
    "        loss_log = ''\n",
    "        val_log = ''\n",
    "        for x in range(len(losses)):\n",
    "            loss_log += f'{losses[x]}\\n'\n",
    "            val_log += f'{validation_loss[x]}\\n'\n",
    "        print('---------------')\n",
    "        # Writing the model results to the log\n",
    "        log = f'{name}\\nInput: {input}\\nOutput: {output}\\nInput layers output: {inputs_output}\\nLayers: {param[1]}\\nNeuron increment:\\n\\ttype: {param[0]}\\n\\tincrement : {param[2]}\\nLosses:\\n{loss_log}\\n'\n",
    "        log += f'Validation loss:\\n{val_log}\\nErrors:\\n\\tmse: {sum(errors[0]) / len(errors[0])}\\n\\tmae: {sum(errors[1]) / len(errors[1])}\\n\\tr2: {sum(errors[2]) / len(errors[2])}\\nAccuracy: {threshold*100}%\\n\\n'\n",
    "        with open(file_name, 'a') as file:\n",
    "            file.write(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9194ac7e-7245-4fc4-8fc2-6cd74b72624e",
   "metadata": {},
   "source": [
    "# Defining the parameters for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "235e9519-80d9-4bd8-a6f1-04b7a7c8a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers value includes the input layer, hidden layers and the output layers.\n",
    "# input is the number of neurons for the input layers input. This is also the number of features used for training\n",
    "# output is the number of neurons for the output layers output. This is regression model, so the output is 1\n",
    "# neuron_increment is the value used to increase or decrease the number of neurons between layers\n",
    "# method is the key used to call the function\n",
    "# inputs_output is the first layers inputs output. This is used to make is easier to plan how the number of neurons change between layers\n",
    "nn_parameters = {'layers': [3], 'input': 31, 'output': 1, 'neuron_increment': [-10], 'method': 'add', 'inputs_output': 30}\n",
    "# Define the hyperparameters for GridSearchCV\n",
    "grid_parameters = {'batch_size': [256], 'optimizer': [torch.optim.Adagrad, torch.optim.SGD], 'lr': [0.01]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f41f4923-2240-4007-95c4-3fe969e0c9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30_add_3_-10\n",
      "Time: 0:03:06.328108\tAccuracy: 5.8404%\n",
      "Training loss: 0.10051791369915009\tValidation loss: 0.0007154945546785602\n",
      "\n",
      "Time: 0:02:59.088400\tAccuracy: 6.6952%\n",
      "Training loss: 0.07258964329957962\tValidation loss: 0.0005996980940471985\n",
      "\n",
      "Time: 0:03:04.550815\tAccuracy: 13.944899999999999%\n",
      "Training loss: 0.0613241009414196\tValidation loss: 0.0005204542587328858\n",
      "\n",
      "Time: 0:03:01.137488\tAccuracy: 18.5348%\n",
      "Training loss: 0.05526987463235855\tValidation loss: 0.00047312659626784444\n",
      "\n",
      "Time: 0:02:58.599494\tAccuracy: 21.6815%\n",
      "Training loss: 0.05147917941212654\tValidation loss: 0.0004427914728561766\n",
      "\n",
      "0:15:09.712451\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "networkLooper(nn_parameters, grid_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05571d-f8e8-441c-94aa-bc3d5777723f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
